import feedparser
import asyncio
import os
import json
import requests
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from google import genai
from telegram import Bot
from telegram.constants import ParseMode
import hashlib
from difflib import SequenceMatcher

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ù‡Ù… ---
TICKER_MAP = {
    "Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI", "Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI", "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH",
    "Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH", "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ Ø¥Ù„ÙŠÙƒØªØ±ÙŠÙƒ": "SWDY", "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ": "SWDY",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¥ÙŠ Ø¥Ù Ø¬ÙŠ Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "HRHO", "Ø§ÙŠ Ø§Ù Ø¬ÙŠ": "HRHO", "Ø­Ø¯ÙŠØ¯ Ø¹Ø²": "ESRS",
    "Ø¹Ø² Ø§Ù„Ø¯Ø®ÙŠÙ„Ø©": "ESRS", "Ø£Ø¨Ùˆ Ù‚ÙŠØ± Ù„Ù„Ø£Ø³Ù…Ø¯Ø©": "ABUK", "ÙÙˆØ±ÙŠ": "FWRY",
    "Ù…ØµØ± Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø£Ø³Ù…Ø¯Ø© - Ù…ÙˆØ¨ÙƒÙˆ": "MFPC", "Ù…ÙˆØ¨ÙƒÙˆ": "MFPC", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ© Ù„ØªØ¯Ø§ÙˆÙ„ Ø§Ù„Ø­Ø§ÙˆÙŠØ§Øª": "ALCN",
    "Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST", "Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST", "Ø¨Ø§Ù„Ù… Ù‡ÙŠÙ„Ø²": "PHDC",
    "Ø³ÙŠØ¯ÙŠ ÙƒØ±ÙŠØ± Ù„Ù„Ø¨ØªØ±ÙˆÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª": "SKPC", "Ø³ÙŠØ¯Ø¨Ùƒ": "SKPC", "Ø£ÙˆØ±Ø§Ø³ÙƒÙˆÙ… ÙƒÙˆÙ†Ø³ØªØ±Ø§ÙƒØ´ÙˆÙ†": "ORAS",
    "Ø¬ÙŠ Ø¨ÙŠ ÙƒÙˆØ±Ø¨": "AUTO", "Ø¥Ø¹Ù…Ø§Ø± Ù…ØµØ±": "EMFD", "Ù…ÙŠÙ†Ø§ Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø³ÙŠØ§Ø­ÙŠ ÙˆØ§Ù„Ø¹Ù‚Ø§Ø±ÙŠ": "MENA",
    "Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø®Ø²Ù ÙˆØ§Ù„ØµÙŠÙ†ÙŠ": "PRCL", "Ø¬Ù†ÙˆØ¨ Ø§Ù„ÙˆØ§Ø¯Ù‰ Ù„Ù„Ø£Ø³Ù…Ù†Øª": "SVCE",
    "Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© Ù„Ù„Ù…Ø­Ø§ØµÙŠÙ„ Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©": "IFAP", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø®Ø²Ù Ø³ÙŠØ±Ø§Ù…ÙŠÙƒØ§": "CERA",
    "Ø§Ù„Ø¹Ø² Ù„Ù„Ø³ÙŠØ±Ø§Ù…ÙŠÙƒ ÙˆØ§Ù„Ø¨ÙˆØ±Ø³Ù„ÙŠÙ†": "ECAP", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø­Ù„ÙŠØ¬ Ø§Ù„Ø£Ù‚Ø·Ø§Ù†": "ACGC",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¹Ø§Ù…Ø± Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "AMER", "Ø§Ù„Ù†ØµØ± Ù„Ù„Ù…Ù„Ø§Ø¨Ø³ ÙˆØ§Ù„Ù…Ù†Ø³ÙˆØ¬Ø§Øª": "KABO",
    "Ø§Ù„Ù…Ø·ÙˆØ±ÙˆÙ† Ø§Ù„Ø¹Ø±Ø¨ Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "ARAB", "Ø·Ø§Ù‚Ø© Ø¹Ø±Ø¨ÙŠØ© Ø´.Ù….Ù…": "TAQA", "Ø§Ù„Ø¹Ø¨ÙˆØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ©": "MEPA",
    "Ø§Ù„Ù…ØµØ±Ù Ø§Ù„Ù…ØªØ­Ø¯": "UBEE", "Ø§Ù„Ø¹Ø¨ÙˆØ± Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ": "OBRI",
    "Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ": "RREI", "Ù…ØµØ±Ù Ø£Ø¨Ùˆ Ø¸Ø¨ÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ - Ù…ØµØ±": "ADIB",
    "Ù†Ù‡Ø± Ø§Ù„Ø®ÙŠØ± Ù„Ù„ØªÙ†Ù…ÙŠØ©": "KRDI", "Ù…Ù…ÙÙŠØ³ Ù„Ù„Ø£Ø¯ÙˆÙŠØ©": "MPCI", "Ù…ØµØ± Ù„Ù„Ø£Ù„ÙˆÙ…Ù†ÙŠÙˆÙ…": "EGAL",
    "Ù…ØµØ± Ù„Ù„Ø£Ø³Ù…Ù†Øª": "MCQE", "Ù…ØµØ± Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ù„Ù„ØµÙ„Ø¨": "ATQA", "Ù…ØµØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø§Ø³ÙƒØ§Ù†": "HELI",
    "Ù…Ø¯ÙŠÙ†Ø© Ù†ØµØ± Ù„Ù„Ø§Ø³ÙƒØ§Ù†": "MASR", "Ù…Ø§ÙƒØ±Ùˆ Ø¬Ø±ÙˆØ¨": "MCRO", "Ù„ÙŠØ³ÙŠÙƒÙˆ Ù…ØµØ±": "LCSW",
    "ÙƒÙˆÙ†ØªÙƒØª Ø§Ù„Ù…Ø§Ù„ÙŠØ©": "CNFN", "ÙØ§Ù„Ù…ÙˆØ± Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "VLMRA", "ØºØ§Ø² Ù…ØµØ±": "EGAS",
    "Ø¹Ø¨ÙˆØ± Ù„Ø§Ù†Ø¯": "OLFI", "Ù…Ø³ØªØ´ÙÙ‰ ÙƒÙ„ÙŠÙˆØ¨Ø§ØªØ±Ø§": "CLHO", "Ø§Ù„Ù‚Ù„Ø¹Ø© Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª": "CCAP",
    "Ø²Ù‡Ø±Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ø¯ÙŠ": "ZMID", "Ø±Ø§ÙŠØ© Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "RAYA", "Ø¯Ø§ÙŠØ³ Ù„Ù„Ù…Ù„Ø§Ø¨Ø³": "DSCW",
    "Ø¬Ù‡ÙŠÙ†Ø© Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©": "JUFO", "Ø¨ÙŠ Ø¥Ù†ÙØ³ØªÙ…Ù†ØªØ³": "BINV", "ÙƒØ±ÙŠØ¯ÙŠ Ø£Ø¬Ø±ÙŠÙƒÙˆÙ„": "CIEB",
    "Ø¨Ù†Ùƒ Ø§Ù„ØªØ¹Ù…ÙŠØ± ÙˆØ§Ù„Ø¥Ø³ÙƒØ§Ù†": "HDBK", "Ø¨Ù„ØªÙˆÙ† Ø§Ù„Ù…Ø§Ù„ÙŠØ©": "BTFH", "Ø¨Ø§ÙŠÙˆÙ†ÙŠØ±Ø² Ø¨Ø±ÙˆØ¨Ø±ØªÙŠØ²": "PRDC",
    "Ø§ÙŠ ÙØ§ÙŠÙ†Ø§Ù†Ø³": "EFIH", "Ø§Ù….Ø§Ù… Ø¬Ø±ÙˆØ¨": "MTIE", "Ø§Ù„Ù†Ø³Ø§Ø¬ÙˆÙ† Ø§Ù„Ø´Ø±Ù‚ÙŠÙˆÙ†": "ORWE",
    "Ø§Ù„Ù…Ù†ØµÙˆØ±Ø© Ù„Ù„Ø¯ÙˆØ§Ø¬Ù†": "MPCO", "Ø§Ù„Ù…Ù„ØªÙ‚Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ": "AMIA", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ": "MPRC",
    "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù„Ù…Ù†ØªØ¬Ø¹Ø§Øª": "EGTS", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù„Ø§ØªØµØ§Ù„Ø§Øª": "ETEL", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù†Ù‚Ù„": "ETRS",
    "Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠØ©": "PHAR"
}

STOCK_KEYWORDS = list(TICKER_MAP.keys()) + [
    "Ø³Ù‡Ù…", "Ø£Ø³Ù‡Ù…", "Ø¨ÙˆØ±ØµØ©", "Ø§Ø±Ø¨Ø§Ø­", "Ø£Ø±Ø¨Ø§Ø­", "Ø®Ø³Ø§Ø¦Ø±", "Ù†ØªØ§Ø¦Ø¬ Ø£Ø¹Ù…Ø§Ù„",
    "Ø²ÙŠØ§Ø¯Ø© Ø±Ø£Ø³ Ù…Ø§Ù„", "ØªÙˆØ²ÙŠØ¹ ÙƒÙˆØ¨ÙˆÙ†", "Ø§Ø³ØªØ­ÙˆØ§Ø°", "Ø§Ù†Ø¯Ù…Ø§Ø¬", "Ø§ÙƒØªØªØ§Ø¨",
    "Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…Ø¬Ù„Ø³ Ø¥Ø¯Ø§Ø±Ø©", "Ø¥ÙØµØ§Ø­", "ØªØ¯Ø§ÙˆÙ„", "Ø§Ù„Ø¨ÙˆØ±ØµØ© Ø§Ù„Ù…ØµØ±ÙŠØ©",
    "EGX", "ÙƒÙˆØ¨ÙˆÙ†", "Ø¬Ù…Ø¹ÙŠØ© Ø¹Ù…ÙˆÙ…ÙŠØ©", "Ù‡ÙŠØ¦Ø© Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©"
]

# --- Ù…Ù„ÙØ§Øª ---
FAIR_VALUES_FILE = "fair_values.json"
SENT_NEWS_DB_FILE = "sent_news_db.json"
FAIR_VALUES_DB = {}

def load_fair_values():
    global FAIR_VALUES_DB
    if os.path.exists(FAIR_VALUES_FILE):
        try:
            with open(FAIR_VALUES_FILE, 'r', encoding='utf-8') as f:
                FAIR_VALUES_DB = json.load(f)
                print(f"âœ… Loaded {len(FAIR_VALUES_DB)} fair values")
        except Exception as e:
            print(f"âš ï¸ Error loading fair values: {e}")

# --- Gemini ---
client = None
model_name = None
if GEMINI_API_KEY:
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
        model_name = "gemini-2.0-flash-exp"
        print(f"âœ… Gemini ready: {model_name}")
    except Exception as e:
        print(f"âš ï¸ Gemini init error: {e}")

# --- Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ---
RSS_FEEDS = [
    "https://www.arabfinance.com/ar/rss/rssbycat/2",
    "https://www.arabfinance.com/ar/rss/rssbycat/3",
    "http://feeds.mubasher.info/ar/EGX/news",
]

MUBASHER_URL = "https://www.mubasher.info/news/eg/pulse/stocks"

# --- Ø¯ÙˆØ§Ù„ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± ---
def is_similar(t1, t2, threshold=0.70):
    if not t1 or not t2:
        return False
    return SequenceMatcher(None, t1.lower().strip(), t2.lower().strip()).ratio() >= threshold

def gen_hash(title, link):
    return hashlib.md5(f"{title}_{link}".encode('utf-8')).hexdigest()

def is_duplicate(title, link, db):
    h = gen_hash(title, link)
    if h in db:
        return True, "hash"
    for _, data in db.items():
        if is_similar(title, data.get('title', '')):
            return True, "similar"
    return False, None

# --- Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def load_db():
    if os.path.exists(SENT_NEWS_DB_FILE):
        try:
            with open(SENT_NEWS_DB_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_db(db):
    try:
        if len(db) > 3000:
            keys = list(db.keys())[-3000:]
            db = {k: db[k] for k in keys}
        with open(SENT_NEWS_DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(db, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Save error: {e}")

# --- Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø³Ù‡Ù… ---
def find_tickers(text):
    found = set()
    text_lower = text.lower()
    for company, ticker in TICKER_MAP.items():
        if company.lower() in text_lower:
            found.add(ticker)
    return list(found)

def get_fv_data(tickers):
    data = {}
    for ticker in tickers:
        if ticker in FAIR_VALUES_DB:
            data[ticker] = FAIR_VALUES_DB[ticker]
    return data

# --- Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ (Ø§Ø­ØªÙŠØ§Ø·ÙŠ) ---
def smart_fallback_analysis(title, fv_data):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø°ÙƒÙŠ Ø¥Ø°Ø§ ÙØ´Ù„ Gemini"""
    
    title_lower = title.lower()
    
    # ÙƒÙ„Ù…Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
    positive_words = ['Ø§Ø±ØªÙØ§Ø¹', 'Ù†Ù…Ùˆ', 'Ø²ÙŠØ§Ø¯Ø©', 'Ø£Ø±Ø¨Ø§Ø­', 'ØªÙˆØ³Ø¹', 'Ø´Ø±Ø§ÙƒØ©', 'Ø§Ø³ØªØ­ÙˆØ§Ø°', 'Ø§ÙƒØªØªØ§Ø¨', 'ØªÙˆØ²ÙŠØ¹']
    # ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ©
    negative_words = ['Ø§Ù†Ø®ÙØ§Ø¶', 'Ø®Ø³Ø§Ø¦Ø±', 'ØªØ±Ø§Ø¬Ø¹', 'Ù‡Ø¨ÙˆØ·', 'ØªØ­Ø°ÙŠØ±']
    
    positive_count = sum(1 for w in positive_words if w in title_lower)
    negative_count = sum(1 for w in negative_words if w in title_lower)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
    numbers = re.findall(r'(\d+(?:\.\d+)?)\s*(?:Ù…Ù„ÙŠØ§Ø±|Ù…Ù„ÙŠÙˆÙ†|Ø£Ù„Ù|%)', title)
    number_text = f"Ø§Ù„Ù‚ÙŠÙ…Ø©: {numbers[0]} {title[title.find(numbers[0]):title.find(numbers[0])+30]}" if numbers else "ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ÙŠ"
    
    # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©
    if fv_data:
        ticker = list(fv_data.keys())[0]
        data = fv_data[ticker]
        upside = data.get('upside_percent', 0)
        curr_price = data.get('current_price', 0)
        fv = data.get('fair_value', 0)
        
        if upside > 15:
            valuation = "ÙØ±ØµØ© Ø´Ø±Ø§Ø¡ Ù‚ÙˆÙŠØ©"
            target = fv * 0.95
            decision = f"Ø´Ø±Ø§Ø¡ ØªØ¯Ø±ÙŠØ¬ÙŠ | Ø§Ù„Ù‡Ø¯Ù {target:.2f} Ø¬.Ù…"
            confidence = 7
        elif upside < -10:
            valuation = "ØªÙ‚ÙŠÙŠÙ… Ù…Ø±ØªÙØ¹"
            target = fv * 1.05
            decision = f"Ø¨ÙŠØ¹ Ø¬Ø²Ø¦ÙŠ | Ø§Ù„Ù‡Ø¯Ù {target:.2f} Ø¬.Ù…"
            confidence = 6
        else:
            valuation = "ØªÙ‚ÙŠÙŠÙ… Ù…ØªÙˆØ§Ø²Ù†"
            target = (curr_price + fv) / 2
            decision = f"Ø§Ø­ØªÙØ§Ø¸ | Ø§Ù„Ù‡Ø¯Ù {target:.2f} Ø¬.Ù…"
            confidence = 5
    else:
        valuation = "Ù…ØªØ§Ø¨Ø¹Ø©"
        decision = "Ø§Ù†ØªØ¸Ø§Ø± Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"
        confidence = 4
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
    if positive_count > negative_count:
        direction = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ - ÙŠØ¯Ø¹Ù… Ø§Ù„Ø£Ø¯Ø§Ø¡"
        impact = f"Ø§Ù„Ø®Ø¨Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠ ÙˆÙŠØ¯Ø¹Ù… Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„Ù†Ù…Ùˆ"
    elif negative_count > positive_count:
        direction = "Ø³Ù„Ø¨ÙŠ - Ø¶ØºØ· Ù…Ø­ØªÙ…Ù„"
        impact = "Ø§Ù„Ø®Ø¨Ø± Ù‚Ø¯ ÙŠØ¤Ø«Ø± Ø³Ù„Ø¨Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù‚ØµÙŠØ± Ø§Ù„Ù…Ø¯Ù‰"
    else:
        direction = "Ù…Ø­Ø§ÙŠØ¯ - Ù…ØªØ§Ø¨Ø¹Ø©"
        impact = "ØªØ£Ø«ÙŠØ± Ù…Ø­Ø¯ÙˆØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù…"
    
    return {
        'impact': impact,
        'number': number_text,
        'direction': direction,
        'confidence': str(confidence),
        'recommendation': decision
    }

# --- Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù€ Gemini ---
async def analyze_with_gemini(title, desc, fv_data):
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù€ Gemini Ù…Ø¹ fallback Ø°ÙƒÙŠ"""
    
    if not client or not model_name:
        return smart_fallback_analysis(title, fv_data)
    
    # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    fv_text = ""
    if fv_data:
        for ticker, data in fv_data.items():
            curr = data.get('current_price', 0)
            fv = data.get('fair_value', 0)
            upside = data.get('upside_percent', 0)
            fv_text += f"Ø§Ù„Ø³Ø¹Ø± {curr:.2f} | Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© {fv:.2f} | ÙØ±ØµØ© {upside:.1f}%"
    
    prompt = f"""Ø­Ù„Ù„ Ø¨Ø¥ÙŠØ¬Ø§Ø²:
Ø§Ù„Ø®Ø¨Ø±: {title}
Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {fv_text}

Ø£Ø¬Ø¨ ÙÙŠ 5 Ø£Ø³Ø·Ø± ÙÙ‚Ø·:
1. Ø§Ù„ØªØ£Ø«ÙŠØ±: [Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©]
2. Ø§Ù„Ø£Ø±Ù‚Ø§Ù…: [Ù†Ø³Ø¨Ø©/Ø±Ù‚Ù…]
3. Ø§Ù„Ø§ØªØ¬Ø§Ù‡: [Ø¥ÙŠØ¬Ø§Ø¨ÙŠ/Ø³Ù„Ø¨ÙŠ/Ù…Ø­Ø§ÙŠØ¯]
4. Ø§Ù„Ø«Ù‚Ø©: [1-10]
5. Ø§Ù„ØªÙˆØµÙŠØ©: [Ø´Ø±Ø§Ø¡/Ø¨ÙŠØ¹/Ø§Ø­ØªÙØ§Ø¸ ÙˆØ§Ù„Ø³Ø¹Ø±]"""

    try:
        response = await asyncio.wait_for(
            asyncio.to_thread(
                client.models.generate_content,
                model=model_name,
                contents=prompt
            ),
            timeout=10.0
        )
        
        text = response.text.strip()
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        
        analysis = {
            'impact': '',
            'number': '',
            'direction': '',
            'confidence': '5',
            'recommendation': ''
        }
        
        for line in lines:
            if any(x in line for x in ['Ø§Ù„ØªØ£Ø«ÙŠØ±', '1.']):
                analysis['impact'] = re.sub(r'^[\d\.\s:]*(Ø§Ù„ØªØ£Ø«ÙŠØ±:?)?\s*', '', line)
            elif any(x in line for x in ['Ø§Ù„Ø£Ø±Ù‚Ø§Ù…', 'Ø§Ù„Ø±Ù‚Ù…', '2.']):
                analysis['number'] = re.sub(r'^[\d\.\s:]*(Ø§Ù„Ø£Ø±Ù‚Ø§Ù…:?|Ø§Ù„Ø±Ù‚Ù…:?)?\s*', '', line)
            elif any(x in line for x in ['Ø§Ù„Ø§ØªØ¬Ø§Ù‡', '3.']):
                analysis['direction'] = re.sub(r'^[\d\.\s:]*(Ø§Ù„Ø§ØªØ¬Ø§Ù‡:?)?\s*', '', line)
            elif any(x in line for x in ['Ø§Ù„Ø«Ù‚Ø©', '4.']):
                m = re.search(r'(\d+)', line)
                if m:
                    analysis['confidence'] = m.group(1)
            elif any(x in line for x in ['Ø§Ù„ØªÙˆØµÙŠØ©', '5.']):
                analysis['recommendation'] = re.sub(r'^[\d\.\s:]*(Ø§Ù„ØªÙˆØµÙŠØ©:?)?\s*', '', line)
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
        if len(analysis['impact']) < 10 or not analysis['recommendation']:
            return smart_fallback_analysis(title, fv_data)
        
        return analysis
        
    except Exception as e:
        print(f"âš ï¸ Gemini â†’ fallback: {str(e)[:40]}")
        return smart_fallback_analysis(title, fv_data)

def is_stock_related(title, desc):
    content = (title + ' ' + desc).lower()
    return any(kw.lower() in content for kw in STOCK_KEYWORDS)

async def process_news(bot, title, desc, link, db):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø¨Ø± ÙˆØ§Ø­Ø¯"""
    
    # ÙØ­Øµ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·
    if not is_stock_related(title, desc):
        return False, "not_related"
    
    # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±
    is_dup, reason = is_duplicate(title, link, db)
    if is_dup:
        return False, f"dup_{reason}"
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø³Ù‡Ù…
    tickers = find_tickers(title + ' ' + desc)
    if not tickers:
        return False, "no_tickers"
    
    # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©
    fv_data = get_fv_data(tickers)
    if not fv_data:
        return False, "no_fv"
    
    print(f"\nğŸ“° {title[:50]}...")
    
    # Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Ù…Ø¹ fallback ØªÙ„Ù‚Ø§Ø¦ÙŠ)
    analysis = await analyze_with_gemini(title, desc, fv_data)
    
    # Ø¥Ø±Ø³Ø§Ù„
    h = gen_hash(title, link)
    sent = 0
    
    for ticker in tickers:
        if ticker not in fv_data:
            continue
        
        data = fv_data[ticker]
        company = data.get('company_names', [ticker])[0]
        curr = data.get('current_price', 0)
        fv = data.get('fair_value', 0)
        upside = data.get('upside_percent', 0)
        
        icon = "ğŸ“ˆ" if upside > 0 else "ğŸ“‰" if upside < 0 else "â†”ï¸"
        
        msg = (
            f"ğŸ›ï¸ <b>{company} (#{ticker})</b>\n"
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            f"ğŸ“Œ {title}\n\n"
            f"ğŸ“Š <b>Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª:</b>\n"
            f"  â€¢ Ø§Ù„Ø³Ø¹Ø±: {curr:.2f} Ø¬.Ù…\n"
            f"  â€¢ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©: {fv:.2f} Ø¬.Ù…\n"
            f"  â€¢ Ø§Ù„ÙØ±ØµØ©: {icon} {abs(upside):.1f}%\n\n"
            f"ğŸ’¡ <b>Ø§Ù„ØªØ­Ù„ÙŠÙ„:</b>\n"
            f"  â€¢ {analysis['impact']}\n"
            f"  â€¢ {analysis['number']}\n"
            f"  â€¢ {analysis['direction']}\n\n"
            f"ğŸ¯ <b>Ø§Ù„ØªÙˆØµÙŠØ© (Ø«Ù‚Ø© {analysis['confidence']}/10):</b>\n"
            f"  {analysis['recommendation']}\n"
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            f"<a href=\"{link}\">Ø§Ù„Ù…ØµØ¯Ø±</a> | {datetime.now().strftime('%H:%M')}"
        )
        
        try:
            await bot.send_message(
                chat_id=CHANNEL_ID,
                text=msg,
                parse_mode=ParseMode.HTML,
                disable_web_page_preview=True
            )
            sent += 1
            print(f"âœ… {ticker}")
            await asyncio.sleep(1.5)  # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
        except Exception as e:
            print(f"âŒ {ticker}: {e}")
    
    if sent > 0:
        db[h] = {
            'title': title,
            'link': link,
            'tickers': tickers,
            'time': datetime.now().isoformat()
        }
        return True, f"sent_{sent}"
    
    return False, "no_send"

async def fetch_mubasher(bot, db):
    """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ù…Ø¨Ø§Ø´Ø±"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(MUBASHER_URL, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙŠ Ø§Ù„ØµÙØ­Ø©
        news_items = soup.find_all('article', class_=re.compile('news|story|item'), limit=20)
        
        if not news_items:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¯ÙŠÙ„Ø©
            news_items = soup.find_all(['div', 'li'], class_=re.compile('news|story|article'), limit=20)
        
        count = 0
        for item in news_items:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                title_tag = item.find(['h2', 'h3', 'h4', 'a'])
                if not title_tag:
                    continue
                
                title = title_tag.get_text(strip=True)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø·
                link_tag = item.find('a', href=True)
                if not link_tag:
                    continue
                
                link = link_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.mubasher.info' + link
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ
                desc_tag = item.find(['p', 'span'], class_=re.compile('desc|summary|excerpt'))
                desc = desc_tag.get_text(strip=True) if desc_tag else ''
                
                if len(title) > 15:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ø¹Ù‚ÙˆÙ„
                    await process_news(bot, title, desc, link, db)
                    count += 1
                    await asyncio.sleep(0.3)
                    
            except Exception as e:
                continue
        
        if count > 0:
            print(f"âœ… Mubasher: processed {count} news")
        else:
            print(f"âš ï¸ Mubasher: no news found")
            
    except Exception as e:
        print(f"âš ï¸ Mubasher error: {str(e)[:50]}")

async def fetch_rss(bot, db):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS"""
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for url in RSS_FEEDS:
        try:
            r = requests.get(url, headers=headers, timeout=12)
            feed = feedparser.parse(r.content)
            
            for entry in feed.entries[:15]:
                await process_news(
                    bot,
                    entry.title,
                    entry.get('summary', ''),
                    entry.link,
                    db
                )
                await asyncio.sleep(0.2)
                
        except Exception as e:
            print(f"âš ï¸ RSS {url[:25]}: {e}")

async def main():
    if not all([TELEGRAM_TOKEN, CHANNEL_ID]):
        print("âŒ Missing tokens!")
        return
    
    load_fair_values()
    db = load_db()
    bot = Bot(token=TELEGRAM_TOKEN)
    
    print(f"\nğŸ¤– EGX Bot v10.0 - Final Edition")
    print(f"ğŸ“Š Gemini: {model_name or 'Smart Fallback Only'}")
    print(f"ğŸ“° DB: {len(db)} news")
    print(f"ğŸŒ Sources: RSS (3) + Mubasher\n")
    
    cycle = 0
    
    while True:
        try:
            cycle += 1
            print(f"\nğŸ”„ Cycle {cycle} - {datetime.now().strftime('%H:%M:%S')}")
            
            # Ø¬Ù„Ø¨ Ù…Ù† RSS
            await fetch_rss(bot, db)
            
            # Ø¬Ù„Ø¨ Ù…Ù† Ù…Ø¨Ø§Ø´Ø±
            await fetch_mubasher(bot, db)
            
            save_db(db)
            
            print(f"âœ… Cycle done. DB: {len(db)}")
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ†ØµÙ
            await asyncio.sleep(90)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Stopping...")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")
            await asyncio.sleep(30)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâœ… Stopped")
