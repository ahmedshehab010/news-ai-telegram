import feedparser
import asyncio
import os
import json
import requests
import random
from bs4 import BeautifulSoup
import google.generativeai as genai
from telegram import Bot
from telegram.constants import ParseMode
import hashlib
from difflib import SequenceMatcher

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- Ù‚Ø§Ù…ÙˆØ³ Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø£Ø³Ù‡Ù… (Ticker-Company Mapping) ---
TICKER_MAP = {
    "Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI",
    "Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH",
    "Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH",
    "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ Ø¥Ù„ÙŠÙƒØªØ±ÙŠÙƒ": "SWDY",
    "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ": "SWDY",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¥ÙŠ Ø¥Ù Ø¬ÙŠ Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "HRHO",
    "Ø§ÙŠ Ø§Ù Ø¬ÙŠ": "HRHO",
    "Ø­Ø¯ÙŠØ¯ Ø¹Ø²": "ESRS",
    "Ø¹Ø² Ø§Ù„Ø¯Ø®ÙŠÙ„Ø©": "ESRS",
    "Ø£Ø¨Ùˆ Ù‚ÙŠØ± Ù„Ù„Ø£Ø³Ù…Ø¯Ø©": "ABUK",
    "ÙÙˆØ±ÙŠ": "FWRY",
    "Ù…ØµØ± Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø£Ø³Ù…Ø¯Ø© - Ù…ÙˆØ¨ÙƒÙˆ": "MFPC",
    "Ù…ÙˆØ¨ÙƒÙˆ": "MFPC",
    "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ© Ù„ØªØ¯Ø§ÙˆÙ„ Ø§Ù„Ø­Ø§ÙˆÙŠØ§Øª": "ALCN",
    "Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST",
    "Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST",
    "Ø¨Ø§Ù„Ù… Ù‡ÙŠÙ„Ø²": "PHDC",
    "Ø³ÙŠØ¯ÙŠ ÙƒØ±ÙŠØ± Ù„Ù„Ø¨ØªØ±ÙˆÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª": "SKPC",
    "Ø³ÙŠØ¯Ø¨Ùƒ": "SKPC",
    "Ø£ÙˆØ±Ø§Ø³ÙƒÙˆÙ… ÙƒÙˆÙ†Ø³ØªØ±Ø§ÙƒØ´ÙˆÙ†": "ORAS",
    "Ø¬ÙŠ Ø¨ÙŠ ÙƒÙˆØ±Ø¨": "AUTO",
    "Ø¥Ø¹Ù…Ø§Ø± Ù…ØµØ±": "EMFD",
    "Ø¬ÙˆØ±Ù…ÙŠÙ‡": "Gourmet_IPO"
}

# --- Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© ---
FAIR_VALUES_FILE = "fair_values.json"
FAIR_VALUES_DB = {}

def load_fair_values():
    global FAIR_VALUES_DB
    if os.path.exists(FAIR_VALUES_FILE):
        try:
            with open(FAIR_VALUES_FILE, "r", encoding="utf-8") as f:
                FAIR_VALUES_DB = json.load(f)
                print(f"âœ… Loaded {len(FAIR_VALUES_DB)} fair value entries.")
        except Exception as e:
            print(f"âš ï¸ Error loading fair_values.json: {e}")
    else:
        print("âš ï¸ fair_values.json not found. Fair value data will be skipped.")

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Gemini Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ø­ØªÙŠØ§Ø·ÙŠ ---
model = None
selected_model_name = None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    MODEL_LIST = ["gemini-1.5-flash", "gemini-pro"]
    for m in MODEL_LIST:
        try:
            model = genai.GenerativeModel(m)
            selected_model_name = m
            print(f"âœ… Model initialized: {selected_model_name}")
            break
        except Exception as e:
            print(f"âš ï¸ Model {m} not available: {str(e)[:50]}")
            continue
if not model:
    print("âŒ Critical Error: No Gemini model found.")

# --- Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ---
RSS_FEEDS = [
    "https://www.arabfinance.com/ar/rss/rssbycat/2",
    "https://www.arabfinance.com/ar/rss/rssbycat/3",
    "http://feeds.mubasher.info/ar/EGX/news",
]
MUBASHER_PULSE_URL = "https://www.mubasher.info/news/eg/pulse/stocks"

# --- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙÙ„ØªØ±Ø© ---
STOCK_KEYWORDS = [
    "Ø³Ù‡Ù…", "Ø£Ø³Ù‡Ù…", "Ø¨ÙˆØ±ØµØ©", "Ø§Ø±Ø¨Ø§Ø­", "Ø£Ø±Ø¨Ø§Ø­", "Ø®Ø³Ø§Ø¦Ø±", "Ù†ØªØ§Ø¦Ø¬ Ø£Ø¹Ù…Ø§Ù„",
    "Ø²ÙŠØ§Ø¯Ø© Ø±Ø£Ø³ Ù…Ø§Ù„", "ØªÙˆØ²ÙŠØ¹ ÙƒÙˆØ¨ÙˆÙ†", "Ø§Ø³ØªØ­ÙˆØ§Ø°", "Ø§Ù†Ø¯Ù…Ø§Ø¬", "Ø§ÙƒØªØªØ§Ø¨",
    "Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…Ø¬Ù„Ø³ Ø¥Ø¯Ø§Ø±Ø©", "Ø¥ÙØµØ§Ø­", "ØªØ¯Ø§ÙˆÙ„", "Ø§Ù„Ø¨ÙˆØ±ØµØ© Ø§Ù„Ù…ØµØ±ÙŠØ©",
    "EGX", "ÙƒÙˆØ¨ÙˆÙ†", "Ø¬Ù…Ø¹ÙŠØ© Ø¹Ù…ÙˆÙ…ÙŠØ©", "Ù‡ÙŠØ¦Ø© Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…ÙˆØ§Ø²Ù†Ø©"
]

# --- Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø§Ù„Ø© ---
SENT_NEWS_DB_FILE = "sent_news_db.json"

# --- Ø¯ÙˆØ§Ù„ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ ---
def is_similar(title1, title2, threshold=0.85):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ´Ø§Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù†ÙŠÙ† Ø¨Ù†Ø³Ø¨Ø© Ù…Ø¹ÙŠÙ†Ø©"""
    return SequenceMatcher(None, title1, title2).ratio() >= threshold

def generate_news_hash(title, link):
    """ØªÙˆÙ„ÙŠØ¯ hash ÙØ±ÙŠØ¯ Ù„Ù„Ø®Ø¨Ø±"""
    return hashlib.md5(f"{title.strip()}_{link.strip()}".encode("utf-8")).hexdigest()

# --- Ø¯ÙˆØ§Ù„ Ø¥Ø¯Ø§Ø±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def load_sent_news_db():
    if os.path.exists(SENT_NEWS_DB_FILE):
        try:
            with open(SENT_NEWS_DB_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"âš ï¸ Error loading DB: {e}. Starting fresh.")
            return {}
    return {}

def save_sent_news_db(db):
    try:
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 500 Ø®Ø¨Ø± ÙÙ‚Ø·
        keys_to_keep = list(db.keys())[-500:]
        trimmed_db = {k: db[k] for k in keys_to_keep}
        with open(SENT_NEWS_DB_FILE, "w", encoding="utf-8") as f:
            json.dump(trimmed_db, f, ensure_ascii=False, indent=2)
    except IOError as e:
        print(f"âš ï¸ Error saving DB: {e}")

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚ ---
def find_tickers(text):
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø±ÙƒØ§Øª ÙÙŠ Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø£ÙƒÙˆØ§Ø¯Ù‡Ø§"""
    found_tickers = set()
    for company, ticker in TICKER_MAP.items():
        if company in text:
            found_tickers.add(f"#{ticker}")
    return list(found_tickers)

def get_fair_value_data(tickers):
    """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
    data = {}
    for ticker_tag in tickers:
        ticker = ticker_tag.replace("#", "")
        if ticker in FAIR_VALUES_DB:
            data[ticker] = FAIR_VALUES_DB[ticker]
    return data

def format_fair_value_for_prompt(fair_value_data):
    """ØªÙ†Ø³ÙŠÙ‚ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© Ù„ØªÙƒÙˆÙ† Ù…Ù‚Ø±ÙˆØ¡Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª"""
    if not fair_value_data:
        return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚ÙŠÙ…Ø© Ø¹Ø§Ø¯Ù„Ø© Ù…ØªØ§Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨Ø±."
    
    formatted_data = []
    for ticker, data in fair_value_data.items():
        company_name = data["company_names"][0] if data["company_names"] else ticker
        fv = f"{data['fair_value']:.2f}" if data['fair_value'] is not None else "N/A"
        upside = f"{data['upside_percent']:.1f}%" if data['upside_percent'] is not None else "N/A"
        valuation = data['valuation'] if data['valuation'] else "N/A"
        
        formatted_data.append(
            f"- {company_name} ({ticker}): Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© {fv} Ø¬.Ù…ØŒ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØµØ¹ÙˆØ¯ {upside}ØŒ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„ÙŠ {valuation}."
        )
    return "\n".join(formatted_data)

async def analyze_news_with_gemini(title, description, tickers, fair_value_data):
    if not model:
        return "âš ï¸ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹."

    prompt = f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø£Ø³Ù‡Ù… Ø£ÙˆÙ„ ÙÙŠ Investing Pro. Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ØªØ§Ù„ÙŠ:

**Ø§Ù„Ø®Ø¨Ø±:** {title}
**Ø§Ù„ØªÙØ§ØµÙŠÙ„:** {description}
**Ø§Ù„Ø£Ø³Ù‡Ù… Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©:** {', '.join(tickers) if tickers else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}

**Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© (Ù…Ù† Investing Pro):**
{format_fair_value_for_prompt(fair_value_data)}

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø¨Ø£Ø³Ù„ÙˆØ¨ Investing Pro):**
1.  **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:** ØªØ£Ø«ÙŠØ± Ø§Ù„Ø®Ø¨Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¨Ø§Ø­ØŒ Ø§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§ØªØŒ ÙˆØ§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¯ÙØªØ±ÙŠØ©.
2.  **Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© (Fair Value):** Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠØ± ØªØ£Ø«ÙŠØ± Ø§Ù„Ø®Ø¨Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© Ù„Ù„Ø³Ù‡Ù…ØŸ (Ø§Ø°ÙƒØ± "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙ‚Ø¯ÙŠØ±" Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©).
3.  **Ø§Ù„ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ø³Ù‡Ù…:** Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¹Ø± (ØµØ¹ÙˆØ¯/Ù‡Ø¨ÙˆØ·)ØŒ ÙˆÙ…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.
4.  **Ø§Ù„ØªÙˆØµÙŠØ©:** ØªÙˆØµÙŠØ© ÙˆØ§Ø¶Ø­Ø© (Ø´Ø±Ø§Ø¡/Ø§Ø­ØªÙØ§Ø¸/Ø¨ÙŠØ¹) Ù…Ø¹ Ø£ÙÙ‚ Ø²Ù…Ù†ÙŠ.

**Ø§Ù„Ø£Ø³Ù„ÙˆØ¨:** Ø§Ø­ØªØ±Ø§ÙÙŠØŒ Ù…ÙˆØ¬Ø²ØŒ Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…."""

    try:
        response = await model.generate_content_async(prompt)
        return response.text.strip()
    except Exception as e:
        return f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)[:80]}"

def is_stock_related(title, description):
    content = (title + " " + description).lower()
    return any(keyword in content for keyword in STOCK_KEYWORDS)

# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„Ø¥Ø±Ø³Ø§Ù„ ---
async def process_and_send(bot, title, description, link, sent_db):
    # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙÙ„ØªØ±Ø© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    if not is_stock_related(title, description):
        return False, "Not stock related"

    # 2. Ù†Ø¸Ø§Ù… Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ
    news_hash = generate_news_hash(title, link)
    if news_hash in sent_db:
        return False, "Duplicate hash"

    for existing_hash, existing_data in sent_db.items():
        if is_similar(title, existing_data["title"]):
            print(f"âš ï¸ Smart De-duplication: Similar title found for '{title[:50]}...'")
            return False, "Similar title"

    # 3. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø£Ø³Ù‡Ù…
    tickers = find_tickers(title + " " + description)

    # 4. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini
    print(f"ğŸ“° Processing: {title[:60]}...")
    fair_value_data = get_fair_value_data(tickers)
    analysis = await analyze_news_with_gemini(title, description, tickers, fair_value_data)

    # 5. ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
    ticker_hashtags = " ".join(tickers) if tickers else ""
    
    # Ø¯Ù…Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©
    fair_value_section = ""
    if fair_value_data:
        fair_value_section += "\n\nğŸ’ <b>Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø§Ù„ÙŠ (Investing Pro Data):</b>\n"
        for ticker, data in fair_value_data.items():
            company_name = data["company_names"][0] if data["company_names"] else ticker
            fv = f"{data['fair_value']:.2f}" if data['fair_value'] is not None else "N/A"
            upside = f"{data['upside_percent']:.1f}%" if data['upside_percent'] is not None else "N/A"
            valuation = data['valuation'] if data['valuation'] else "N/A"
            
            fair_value_section += (
                f"â€¢ <b>{company_name} ({ticker}):</b>\n"
                f"  - Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©: {fv} Ø¬.Ù…\n"
                f"  - Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØµØ¹ÙˆØ¯: {upside}\n"
                f"  - Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„ÙŠ: {valuation}\n"
            )

    message = (
        f"ğŸ›ï¸ <b>ØªÙ‚Ø±ÙŠØ± Ø¨Ø­ÙˆØ« Ø§Ù„Ø¨ÙˆØ±ØµØ© Ø§Ù„Ù…ØµØ±ÙŠØ©</b>\n"
        f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        f"ğŸ“Œ <b>Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:</b>\n{title}\n\n"
        f"ğŸ”¬ <b>Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø­Ø«ÙŠ Ø§Ù„Ù…ØªØ¹Ù…Ù‚ (Investing Pro Style):</b>\n{analysis}\n"
        f"{fair_value_section}\n"
        f"ğŸ”— <a href='{link}'>Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ØµÙ„ÙŠ</a>\n"
        f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        f"{ticker_hashtags}"
    )

    # 6. Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ ÙˆØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        await bot.send_message(chat_id=CHANNEL_ID, text=message, parse_mode=ParseMode.HTML)
        sent_db[news_hash] = {"title": title, "link": link}
        print(f"âœ… Sent: {title[:60]}...")
        return True, "Sent"
    except Exception as e:
        print(f"âŒ Error sending message: {e}")
        return False, f"Telegram error: {e}"

# --- Ø¯ÙˆØ§Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ---
async def fetch_rss_feeds(bot, sent_db):
    headers = {"User-Agent": "Mozilla/5.0"}
    for url in RSS_FEEDS:
        try:
            response = requests.get(url, headers=headers, timeout=15)
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:15]:
                await process_and_send(bot, entry.title, entry.get("summary", ""), entry.link, sent_db)
                await asyncio.sleep(1)
        except Exception as e:
            print(f"âš ï¸ Error fetching RSS {url[:50]}: {e}")

async def fetch_pulse(bot, sent_db):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(MUBASHER_PULSE_URL, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, "html.parser")
        for a in soup.find_all("a", href=True, limit=10):
            if "/news/" in a["href"] and len(a.get_text(strip=True)) > 20:
                link = a["href"] if a["href"].startswith("http") else "https://www.mubasher.info" + a["href"]
                await process_and_send(bot, a.get_text(strip=True), "Ø®Ø¨Ø± Ø¹Ø§Ø¬Ù„ Ù…Ù† Ù†Ø¨Ø¶ Ø§Ù„Ø£Ø³Ù‡Ù…", link, sent_db)
                await asyncio.sleep(1)
    except Exception as e:
        print(f"âš ï¸ Error fetching Pulse: {e}")

# --- Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
async def main():
    if not all([TELEGRAM_TOKEN, CHANNEL_ID, GEMINI_API_KEY]):
        print("âŒ Missing environment variables!")
        return

    bot = Bot(token=TELEGRAM_TOKEN)
    sent_db = load_sent_news_db()

    print(f"\nğŸ¤– EGX News Bot v4 Started | Model: {selected_model_name or 'N/A'} | Tracked: {len(sent_db)}")
    load_fair_values()

    await fetch_rss_feeds(bot, sent_db)
    await fetch_pulse(bot, sent_db)

    save_sent_news_db(sent_db)
    print(f"\nâœ… Cycle completed. Total tracked news: {len(sent_db)}\n")

if __name__ == "__main__":
    asyncio.run(main())
