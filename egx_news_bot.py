import feedparser
import asyncio
import os
import json
import requests
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from google import genai
from telegram import Bot
from telegram.constants import ParseMode
import hashlib
from difflib import SequenceMatcher

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ØªØ´ØºÙŠÙ„/Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø³Ø§Ø¹ÙŠ
ENABLE_HOURLY_SUMMARY = False  # ØºÙŠØ±Ù‡Ø§ Ù„Ù€ True Ù„ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù…Ù„Ø®Øµ

# --- Ù‚Ø§Ù…ÙˆØ³ Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø£Ø³Ù‡Ù… ---
TICKER_MAP = {
    "Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI", "Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI", "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH",
    "Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH", "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ Ø¥Ù„ÙŠÙƒØªØ±ÙŠÙƒ": "SWDY", "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ": "SWDY",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¥ÙŠ Ø¥Ù Ø¬ÙŠ Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "HRHO", "Ø§ÙŠ Ø§Ù Ø¬ÙŠ": "HRHO", "Ø­Ø¯ÙŠØ¯ Ø¹Ø²": "ESRS",
    "Ø¹Ø² Ø§Ù„Ø¯Ø®ÙŠÙ„Ø©": "ESRS", "Ø£Ø¨Ùˆ Ù‚ÙŠØ± Ù„Ù„Ø£Ø³Ù…Ø¯Ø©": "ABUK", "ÙÙˆØ±ÙŠ": "FWRY",
    "Ù…ØµØ± Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø£Ø³Ù…Ø¯Ø© - Ù…ÙˆØ¨ÙƒÙˆ": "MFPC", "Ù…ÙˆØ¨ÙƒÙˆ": "MFPC", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ© Ù„ØªØ¯Ø§ÙˆÙ„ Ø§Ù„Ø­Ø§ÙˆÙŠØ§Øª": "ALCN",
    "Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST", "Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST", "Ø¨Ø§Ù„Ù… Ù‡ÙŠÙ„Ø²": "PHDC",
    "Ø³ÙŠØ¯ÙŠ ÙƒØ±ÙŠØ± Ù„Ù„Ø¨ØªØ±ÙˆÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª": "SKPC", "Ø³ÙŠØ¯Ø¨Ùƒ": "SKPC", "Ø£ÙˆØ±Ø§Ø³ÙƒÙˆÙ… ÙƒÙˆÙ†Ø³ØªØ±Ø§ÙƒØ´ÙˆÙ†": "ORAS",
    "Ø¬ÙŠ Ø¨ÙŠ ÙƒÙˆØ±Ø¨": "AUTO", "Ø¥Ø¹Ù…Ø§Ø± Ù…ØµØ±": "EMFD", "Ù…ÙŠÙ†Ø§ Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø³ÙŠØ§Ø­ÙŠ ÙˆØ§Ù„Ø¹Ù‚Ø§Ø±ÙŠ": "MENA",
    "Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø®Ø²Ù ÙˆØ§Ù„ØµÙŠÙ†ÙŠ": "PRCL", "Ø¬Ù†ÙˆØ¨ Ø§Ù„ÙˆØ§Ø¯Ù‰ Ù„Ù„Ø£Ø³Ù…Ù†Øª": "SVCE",
    "Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© Ù„Ù„Ù…Ø­Ø§ØµÙŠÙ„ Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©": "IFAP", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø®Ø²Ù Ø³ÙŠØ±Ø§Ù…ÙŠÙƒØ§": "CERA",
    "Ø§Ù„Ø¹Ø² Ù„Ù„Ø³ÙŠØ±Ø§Ù…ÙŠÙƒ ÙˆØ§Ù„Ø¨ÙˆØ±Ø³Ù„ÙŠÙ†": "ECAP", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø­Ù„ÙŠØ¬ Ø§Ù„Ø£Ù‚Ø·Ø§Ù†": "ACGC",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¹Ø§Ù…Ø± Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "AMER", "Ø§Ù„Ù†ØµØ± Ù„Ù„Ù…Ù„Ø§Ø¨Ø³ ÙˆØ§Ù„Ù…Ù†Ø³ÙˆØ¬Ø§Øª": "KABO",
    "Ø§Ù„Ù…Ø·ÙˆØ±ÙˆÙ† Ø§Ù„Ø¹Ø±Ø¨ Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "ARAB", "Ø·Ø§Ù‚Ø© Ø¹Ø±Ø¨ÙŠØ© Ø´.Ù….Ù…": "TAQA", "Ø§Ù„Ø¹Ø¨ÙˆØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ©": "MEPA",
    "Ø§Ù„Ù…ØµØ±Ù Ø§Ù„Ù…ØªØ­Ø¯": "UBEE", "Ø§Ù„Ø¹Ø¨ÙˆØ± Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ": "OBRI",
    "Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ": "RREI", "Ù…ØµØ±Ù Ø£Ø¨Ùˆ Ø¸Ø¨ÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ - Ù…ØµØ±": "ADIB",
    "Ù†Ù‡Ø± Ø§Ù„Ø®ÙŠØ± Ù„Ù„ØªÙ†Ù…ÙŠØ©": "KRDI", "Ù…Ù…ÙÙŠØ³ Ù„Ù„Ø£Ø¯ÙˆÙŠØ©": "MPCI", "Ù…ØµØ± Ù„Ù„Ø£Ù„ÙˆÙ…Ù†ÙŠÙˆÙ…": "EGAL",
    "Ù…ØµØ± Ù„Ù„Ø£Ø³Ù…Ù†Øª": "MCQE", "Ù…ØµØ± Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ù„Ù„ØµÙ„Ø¨": "ATQA", "Ù…ØµØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø§Ø³ÙƒØ§Ù†": "HELI",
    "Ù…Ø¯ÙŠÙ†Ø© Ù†ØµØ± Ù„Ù„Ø§Ø³ÙƒØ§Ù†": "MASR", "Ù…Ø§ÙƒØ±Ùˆ Ø¬Ø±ÙˆØ¨": "MCRO", "Ù„ÙŠØ³ÙŠÙƒÙˆ Ù…ØµØ±": "LCSW",
    "ÙƒÙˆÙ†ØªÙƒØª Ø§Ù„Ù…Ø§Ù„ÙŠØ©": "CNFN", "ÙØ§Ù„Ù…ÙˆØ± Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "VLMRA", "ØºØ§Ø² Ù…ØµØ±": "EGAS",
    "Ø¹Ø¨ÙˆØ± Ù„Ø§Ù†Ø¯": "OLFI", "Ù…Ø³ØªØ´ÙÙ‰ ÙƒÙ„ÙŠÙˆØ¨Ø§ØªØ±Ø§": "CLHO", "Ø§Ù„Ù‚Ù„Ø¹Ø© Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª": "CCAP",
    "Ø²Ù‡Ø±Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ø¯ÙŠ": "ZMID", "Ø±Ø§ÙŠØ© Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "RAYA", "Ø¯Ø§ÙŠØ³ Ù„Ù„Ù…Ù„Ø§Ø¨Ø³": "DSCW",
    "Ø¬Ù‡ÙŠÙ†Ø© Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©": "JUFO", "Ø¨ÙŠ Ø¥Ù†ÙØ³ØªÙ…Ù†ØªØ³": "BINV", "ÙƒØ±ÙŠØ¯ÙŠ Ø£Ø¬Ø±ÙŠÙƒÙˆÙ„": "CIEB",
    "Ø¨Ù†Ùƒ Ø§Ù„ØªØ¹Ù…ÙŠØ± ÙˆØ§Ù„Ø¥Ø³ÙƒØ§Ù†": "HDBK", "Ø¨Ù„ØªÙˆÙ† Ø§Ù„Ù…Ø§Ù„ÙŠØ©": "BTFH", "Ø¨Ø§ÙŠÙˆÙ†ÙŠØ±Ø² Ø¨Ø±ÙˆØ¨Ø±ØªÙŠØ²": "PRDC",
    "Ø§ÙŠ ÙØ§ÙŠÙ†Ø§Ù†Ø³": "EFIH", "Ø§Ù….Ø§Ù… Ø¬Ø±ÙˆØ¨": "MTIE", "Ø§Ù„Ù†Ø³Ø§Ø¬ÙˆÙ† Ø§Ù„Ø´Ø±Ù‚ÙŠÙˆÙ†": "ORWE",
    "Ø§Ù„Ù…Ù†ØµÙˆØ±Ø© Ù„Ù„Ø¯ÙˆØ§Ø¬Ù†": "MPCO", "Ø§Ù„Ù…Ù„ØªÙ‚Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ": "AMIA", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ": "MPRC",
    "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù„Ù…Ù†ØªØ¬Ø¹Ø§Øª": "EGTS", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù„Ø§ØªØµØ§Ù„Ø§Øª": "ETEL", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù†Ù‚Ù„": "ETRS",
    "Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠØ©": "PHAR"
}

# --- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ---
STOCK_KEYWORDS = list(TICKER_MAP.keys()) + [
    "Ø³Ù‡Ù…", "Ø£Ø³Ù‡Ù…", "Ø¨ÙˆØ±ØµØ©", "Ø§Ø±Ø¨Ø§Ø­", "Ø£Ø±Ø¨Ø§Ø­", "Ø®Ø³Ø§Ø¦Ø±", "Ù†ØªØ§Ø¦Ø¬ Ø£Ø¹Ù…Ø§Ù„",
    "Ø²ÙŠØ§Ø¯Ø© Ø±Ø£Ø³ Ù…Ø§Ù„", "ØªÙˆØ²ÙŠØ¹ ÙƒÙˆØ¨ÙˆÙ†", "Ø§Ø³ØªØ­ÙˆØ§Ø°", "Ø§Ù†Ø¯Ù…Ø§Ø¬", "Ø§ÙƒØªØªØ§Ø¨",
    "Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…Ø¬Ù„Ø³ Ø¥Ø¯Ø§Ø±Ø©", "Ø¥ÙØµØ§Ø­", "ØªØ¯Ø§ÙˆÙ„", "Ø§Ù„Ø¨ÙˆØ±ØµØ© Ø§Ù„Ù…ØµØ±ÙŠØ©",
    "EGX", "ÙƒÙˆØ¨ÙˆÙ†", "Ø¬Ù…Ø¹ÙŠØ© Ø¹Ù…ÙˆÙ…ÙŠØ©", "Ù‡ÙŠØ¦Ø© Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…ÙˆØ§Ø²Ù†Ø©"
]

# --- Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
FAIR_VALUES_FILE = "fair_values.json"
SENT_NEWS_DB_FILE = "sent_news_db.json"
HOURLY_SUMMARY_FILE = "hourly_news.json"

FAIR_VALUES_DB = {}

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def load_fair_values():
    global FAIR_VALUES_DB
    if os.path.exists(FAIR_VALUES_FILE):
        try:
            with open(FAIR_VALUES_FILE, 'r', encoding='utf-8') as f:
                FAIR_VALUES_DB = json.load(f)
                print(f"âœ… Loaded {len(FAIR_VALUES_DB)} fair value entries.")
        except Exception as e:
            print(f"âš ï¸ Error loading fair_values.json: {e}")
    else:
        print("âš ï¸ fair_values.json not found.")

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Gemini ---
client = None
selected_model_name = None
if GEMINI_API_KEY:
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
        selected_model_name = "gemini-2.0-flash-exp"
        print(f"âœ… Gemini initialized: {selected_model_name}")
    except Exception as e:
        print(f"âš ï¸ Gemini error: {e}")

# --- Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ---
RSS_FEEDS = [
    "https://www.arabfinance.com/ar/rss/rssbycat/2",
    "https://www.arabfinance.com/ar/rss/rssbycat/3",
    "http://feeds.mubasher.info/ar/EGX/news",
]

# --- Ø¯ÙˆØ§Ù„ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© ---
def is_similar(title1, title2, threshold=0.75):
    """ÙØ­Øµ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø¹Ù†ÙˆØ§Ù†ÙŠÙ† - Ø¹ØªØ¨Ø© 75%"""
    if not title1 or not title2:
        return False
    return SequenceMatcher(None, title1.lower().strip(), title2.lower().strip()).ratio() >= threshold

def generate_news_hash(title, link):
    """Ø¥Ù†Ø´Ø§Ø¡ hash ÙØ±ÙŠØ¯ Ù„Ù„Ø®Ø¨Ø±"""
    return hashlib.md5(f"{title.strip()}_{link.strip()}".encode('utf-8')).hexdigest()

def is_duplicate(title, link, sent_db):
    """ÙØ­Øµ Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙƒØ±Ø§Ø±"""
    news_hash = generate_news_hash(title, link)
    
    # ÙØ­Øµ Hash Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
    if news_hash in sent_db:
        return True, "duplicate_hash"
    
    # ÙØ­Øµ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø±Ø³Ù„Ø©
    for existing_hash, existing_data in sent_db.items():
        existing_title = existing_data.get('title', '')
        if is_similar(title, existing_title):
            return True, f"similar_to: {existing_title[:30]}"
    
    return False, None

# --- Ø¥Ø¯Ø§Ø±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def load_sent_news_db():
    if os.path.exists(SENT_NEWS_DB_FILE):
        try:
            with open(SENT_NEWS_DB_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_sent_news_db(db):
    try:
        # Ø§Ø­ØªÙØ¸ Ø¨Ø¢Ø®Ø± 2000 Ø®Ø¨Ø±
        if len(db) > 2000:
            keys_to_keep = list(db.keys())[-2000:]
            db = {k: db[k] for k in keys_to_keep}
        with open(SENT_NEWS_DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(db, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Error saving DB: {e}")

def load_hourly_news():
    if os.path.exists(HOURLY_SUMMARY_FILE):
        try:
            with open(HOURLY_SUMMARY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def save_hourly_news(news_list):
    try:
        with open(HOURLY_SUMMARY_FILE, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Error saving hourly: {e}")

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ---
def find_tickers(text):
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ù…ÙˆØ² Ø§Ù„Ø£Ø³Ù‡Ù… ÙÙŠ Ø§Ù„Ù†Øµ"""
    found_tickers = set()
    text_lower = text.lower()
    for company, ticker in TICKER_MAP.items():
        if company.lower() in text_lower:
            found_tickers.add(ticker)
    return list(found_tickers)

def get_fair_value_data(tickers):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©"""
    data = {}
    for ticker in tickers:
        if ticker in FAIR_VALUES_DB:
            data[ticker] = FAIR_VALUES_DB[ticker]
    return data

async def analyze_news_with_gemini(title, description, fair_value_data):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­Ø³Ù‘Ù†Ø©"""
    
    if not client or not selected_model_name:
        print("âš ï¸ Gemini not available")
        return None
    
    # ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©
    fv_summary = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"
    if fair_value_data:
        fv_parts = []
        for ticker, data in fair_value_data.items():
            company = data.get('company_names', [ticker])[0]
            curr = data.get('current_price', 0)
            fv = data.get('fair_value', 0)
            upside = data.get('upside_percent', 0)
            fv_parts.append(f"{company}: Ø³Ø¹Ø± {curr:.2f} Ø¬.Ù… | Ù‚ÙŠÙ…Ø© Ø¹Ø§Ø¯Ù„Ø© {fv:.2f} Ø¬.Ù… | ØµØ¹ÙˆØ¯ {upside:.1f}%")
        fv_summary = " | ".join(fv_parts)
    
    # Prompt Ù…Ø­Ø³Ù‘Ù† ÙˆØ£Ù‚ØµØ±
    prompt = f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ø§Ù„ÙŠ. Ø­Ù„Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨Ø± Ø¨Ø¥ÙŠØ¬Ø§Ø² Ø´Ø¯ÙŠØ¯:

Ø§Ù„Ø®Ø¨Ø±: {title}
Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {fv_summary}

Ø£Ø¹Ø·Ù†ÙŠ ÙÙ‚Ø· (Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø©):
1. Ø§Ù„ØªØ£Ø«ÙŠØ±: [Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù† ØªØ£Ø«ÙŠØ± Ø§Ù„Ø®Ø¨Ø±]
2. Ø§Ù„Ø±Ù‚Ù…: [Ù†Ø³Ø¨Ø© Ø£Ùˆ Ø±Ù‚Ù… Ù…ØªÙˆÙ‚Ø¹]
3. Ø§Ù„Ø§ØªØ¬Ø§Ù‡: [Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø£Ùˆ Ø³Ù„Ø¨ÙŠ Ø£Ùˆ Ù…Ø­Ø§ÙŠØ¯]
4. Ø§Ù„Ø«Ù‚Ø©: [Ø±Ù‚Ù… Ù…Ù† 1 Ø¥Ù„Ù‰ 10]
5. Ø§Ù„ØªÙˆØµÙŠØ©: [Ø´Ø±Ø§Ø¡ Ø£Ùˆ Ø¨ÙŠØ¹ Ø£Ùˆ Ø§Ø­ØªÙØ§Ø¸ ÙˆØ§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù]

Ù…Ø«Ø§Ù„:
1. Ø§Ù„ØªØ£Ø«ÙŠØ±: Ø²ÙŠØ§Ø¯Ø© Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ ØªØ¯Ø¹Ù… Ø§Ù„ØªÙˆØ³Ø¹ ÙˆØªØ­Ø³Ù† Ø§Ù„Ø³ÙŠÙˆÙ„Ø©
2. Ø§Ù„Ø±Ù‚Ù…: Ù†Ù…Ùˆ Ù…ØªÙˆÙ‚Ø¹ 20-25% ÙÙŠ Ø§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§Øª
3. Ø§Ù„Ø§ØªØ¬Ø§Ù‡: Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ù…ØªÙˆØ³Ø·
4. Ø§Ù„Ø«Ù‚Ø©: 7
5. Ø§Ù„ØªÙˆØµÙŠØ©: Ø´Ø±Ø§Ø¡ | Ø§Ù„Ù‡Ø¯Ù 0.25 Ø¬.Ù…"""

    try:
        response = await asyncio.wait_for(
            asyncio.to_thread(
                client.models.generate_content,
                model=selected_model_name,
                contents=prompt
            ),
            timeout=15.0  # timeout Ø¨Ø¹Ø¯ 15 Ø«Ø§Ù†ÙŠØ©
        )
        
        text = response.text.strip()
        print(f"ğŸ“ Gemini response: {text[:100]}...")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø©
        lines = text.split('\n')
        analysis = {
            'impact': '',
            'number': '',
            'direction': '',
            'confidence': '5',
            'recommendation': 'Ø§Ù†ØªØ¸Ø§Ø± | ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ'
        }
        
        for line in lines:
            line = line.strip()
            if 'Ø§Ù„ØªØ£Ø«ÙŠØ±:' in line or line.startswith('1.'):
                analysis['impact'] = re.sub(r'^[\d\.]+\s*(Ø§Ù„ØªØ£Ø«ÙŠØ±:)?\s*', '', line).strip()
            elif 'Ø§Ù„Ø±Ù‚Ù…:' in line or line.startswith('2.'):
                analysis['number'] = re.sub(r'^[\d\.]+\s*(Ø§Ù„Ø±Ù‚Ù…:)?\s*', '', line).strip()
            elif 'Ø§Ù„Ø§ØªØ¬Ø§Ù‡:' in line or line.startswith('3.'):
                analysis['direction'] = re.sub(r'^[\d\.]+\s*(Ø§Ù„Ø§ØªØ¬Ø§Ù‡:)?\s*', '', line).strip()
            elif 'Ø§Ù„Ø«Ù‚Ø©:' in line or line.startswith('4.'):
                conf_match = re.search(r'(\d+)', line)
                if conf_match:
                    analysis['confidence'] = conf_match.group(1)
            elif 'Ø§Ù„ØªÙˆØµÙŠØ©:' in line or line.startswith('5.'):
                analysis['recommendation'] = re.sub(r'^[\d\.]+\s*(Ø§Ù„ØªÙˆØµÙŠØ©:)?\s*', '', line).strip()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        if not analysis['impact'] or len(analysis['impact']) < 10:
            print("âš ï¸ Analysis too short, skipping")
            return None
            
        return analysis
        
    except asyncio.TimeoutError:
        print("âš ï¸ Gemini timeout")
        return None
    except Exception as e:
        print(f"âš ï¸ Gemini error: {str(e)[:100]}")
        return None

def is_stock_related(title, description):
    """ÙØ­Øµ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø®Ø¨Ø± Ø¨Ø§Ù„Ø£Ø³Ù‡Ù…"""
    content = (title + ' ' + description).lower()
    return any(keyword.lower() in content for keyword in STOCK_KEYWORDS)

async def process_and_send(bot, title, description, link, sent_db, hourly_news):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø®Ø¨Ø±"""
    
    # ÙØ­Øµ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§Ù„Ø£Ø³Ù‡Ù…
    if not is_stock_related(title, description):
        return False, "Not stock related"
    
    # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„
    is_dup, dup_reason = is_duplicate(title, link, sent_db)
    if is_dup:
        return False, f"Duplicate: {dup_reason}"
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø³Ù‡Ù…
    tickers = find_tickers(title + ' ' + description)
    if not tickers:
        return False, "No tickers"
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©
    fair_value_data = get_fair_value_data(tickers)
    if not fair_value_data:
        return False, "No fair value data"
    
    print(f"\nğŸ“° Processing: {title[:60]}")
    
    # Ø§Ù„ØªØ­Ù„ÙŠÙ„
    analysis = await analyze_news_with_gemini(title, description, fair_value_data)
    
    if not analysis:
        print("âš ï¸ Analysis failed, skipping news")
        return False, "Analysis failed"
    
    # Ø¥Ø±Ø³Ø§Ù„ Ù„ÙƒÙ„ Ø³Ù‡Ù…
    news_hash = generate_news_hash(title, link)
    sent_count = 0
    
    for ticker in tickers:
        if ticker not in fair_value_data:
            continue
        
        data = fair_value_data[ticker]
        company_name = data.get('company_names', [ticker])[0]
        
        curr_price = data.get('current_price', 0)
        current_price_str = f"{curr_price:.2f}" if curr_price else "N/A"
        
        fv = data.get('fair_value', 0)
        fv_str = f"{fv:.2f}" if fv else "N/A"
        
        upside_percent = data.get('upside_percent', 0)
        upside_icon = "ğŸ“ˆ" if upside_percent > 0 else ("ğŸ“‰" if upside_percent < 0 else "â†”ï¸")
        upside_str = f"{abs(upside_percent):.1f}%"
        
        message = (
            f"ğŸ›ï¸ <b>ØªØ­Ù„ÙŠÙ„: {company_name} (#{ticker})</b>\n"
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            f"ğŸ“Œ <b>Ø§Ù„Ø®Ø¨Ø±:</b> {title}\n\n"
            f"ğŸ“Š <b>Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª:</b>\n"
            f"  â€¢ Ø§Ù„Ø³Ø¹Ø±: {current_price_str} Ø¬.Ù…\n"
            f"  â€¢ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©: {fv_str} Ø¬.Ù…\n"
            f"  â€¢ Ø§Ù„ÙØ±ØµØ©: {upside_icon} {upside_str}\n\n"
            f"ğŸ’¡ <b>Ø§Ù„ØªØ­Ù„ÙŠÙ„:</b>\n"
            f"  â€¢ {analysis['impact']}\n"
            f"  â€¢ {analysis['number']}\n"
            f"  â€¢ {analysis['direction']}\n\n"
            f"ğŸ¯ <b>Ø§Ù„ØªÙˆØµÙŠØ© (Ø«Ù‚Ø© {analysis['confidence']}/10):</b>\n"
            f"  {analysis['recommendation']}\n"
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            f"<a href=\"{link}\">Ø§Ù„Ù…ØµØ¯Ø±</a> | {datetime.now().strftime('%H:%M')}"
        )
        
        try:
            await bot.send_message(
                chat_id=CHANNEL_ID,
                text=message,
                parse_mode=ParseMode.HTML,
                disable_web_page_preview=True
            )
            
            sent_count += 1
            print(f"âœ… Sent: {ticker}")
            await asyncio.sleep(2)
            
        except Exception as e:
            print(f"âŒ Send error {ticker}: {e}")
    
    if sent_count > 0:
        # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        sent_db[news_hash] = {
            'title': title,
            'link': link,
            'tickers': tickers,
            'timestamp': datetime.now().isoformat()
        }
        
        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø³Ø§Ø¹ÙŠ
        if ENABLE_HOURLY_SUMMARY:
            for ticker in tickers:
                hourly_news.append({
                    'title': title,
                    'ticker': ticker,
                    'timestamp': datetime.now().isoformat()
                })
        
        return True, f"Sent to {sent_count} ticker(s)"
    
    return False, "No messages sent"

async def send_hourly_summary(bot, hourly_news):
    """Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ø®Øµ Ø³Ø§Ø¹ÙŠ"""
    if not hourly_news or not ENABLE_HOURLY_SUMMARY:
        return
    
    ticker_groups = {}
    for news in hourly_news:
        ticker = news['ticker']
        if ticker not in ticker_groups:
            ticker_groups[ticker] = []
        ticker_groups[ticker].append(news)
    
    summary = f"ğŸ“Š <b>Ù…Ù„Ø®Øµ Ø§Ù„Ø³Ø§Ø¹Ø©</b>\n"
    summary += f"â° {datetime.now().strftime('%H:%M')}\n"
    summary += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    
    for ticker, news_list in list(ticker_groups.items())[:10]:
        summary += f"<b>#{ticker}</b>: {len(news_list)} Ø®Ø¨Ø±\n"
    
    summary += f"\nğŸ“ˆ Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(hourly_news)} Ø®Ø¨Ø±"
    
    try:
        await bot.send_message(
            chat_id=CHANNEL_ID,
            text=summary,
            parse_mode=ParseMode.HTML
        )
        print("âœ… Summary sent")
    except Exception as e:
        print(f"âŒ Summary error: {e}")

async def fetch_rss_feeds(bot, sent_db, hourly_news):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for url in RSS_FEEDS:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            feed = feedparser.parse(response.content)
            
            for entry in feed.entries[:15]:
                await process_and_send(
                    bot,
                    entry.title,
                    entry.get('summary', ''),
                    entry.link,
                    sent_db,
                    hourly_news
                )
                await asyncio.sleep(0.3)
                
        except Exception as e:
            print(f"âš ï¸ RSS error {url[:30]}: {e}")

async def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    if not all([TELEGRAM_TOKEN, CHANNEL_ID, GEMINI_API_KEY]):
        print("âŒ Missing environment variables!")
        return
    
    load_fair_values()
    sent_db = load_sent_news_db()
    hourly_news = load_hourly_news()
    
    bot = Bot(token=TELEGRAM_TOKEN)
    
    print(f"\nğŸ¤– EGX News Bot v8.0")
    print(f"ğŸ“Š Model: {selected_model_name or 'N/A'}")
    print(f"ğŸ“° Tracked: {len(sent_db)}\n")
    
    last_summary = datetime.now()
    cycle = 0
    
    while True:
        try:
            cycle += 1
            print(f"\nğŸ”„ Cycle {cycle} - {datetime.now().strftime('%H:%M:%S')}")
            
            await fetch_rss_feeds(bot, sent_db, hourly_news)
            
            # Ù…Ù„Ø®Øµ Ø³Ø§Ø¹ÙŠ
            if ENABLE_HOURLY_SUMMARY and (datetime.now() - last_summary) >= timedelta(hours=1):
                await send_hourly_summary(bot, hourly_news)
                hourly_news = []
                last_summary = datetime.now()
            
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            save_sent_news_db(sent_db)
            if ENABLE_HOURLY_SUMMARY:
                save_hourly_news(hourly_news)
            
            print(f"âœ… Cycle {cycle} done. DB: {len(sent_db)}")
            
            # Ø§Ù†ØªØ¸Ø§Ø± 2 Ø¯Ù‚ÙŠÙ‚Ø©
            await asyncio.sleep(120)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Stopping...")
            break
        except Exception as e:
            print(f"âŒ Main error: {e}")
            await asyncio.sleep(30)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâœ… Bot stopped")
