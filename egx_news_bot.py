import feedparser
import asyncio
import os
import json
import requests
import re
from bs4 import BeautifulSoup
import google.generativeai as genai
from google.generativeai.errors import APIError
from telegram import Bot
from telegram.constants import ParseMode
import hashlib
from difflib import SequenceMatcher

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- Ù‚Ø§Ù…ÙˆØ³ Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø£Ø³Ù‡Ù… (Ticker-Company Mapping) ---
TICKER_MAP = {
    "Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI", "Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ": "COMI", "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH",
    "Ø·Ù„Ø¹Øª Ù…ØµØ·ÙÙ‰": "TMGH", "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ Ø¥Ù„ÙŠÙƒØªØ±ÙŠÙƒ": "SWDY", "Ø§Ù„Ø³ÙˆÙŠØ¯ÙŠ": "SWDY",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¥ÙŠ Ø¥Ù Ø¬ÙŠ Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "HRHO", "Ø§ÙŠ Ø§Ù Ø¬ÙŠ": "HRHO", "Ø­Ø¯ÙŠØ¯ Ø¹Ø²": "ESRS",
    "Ø¹Ø² Ø§Ù„Ø¯Ø®ÙŠÙ„Ø©": "ESRS", "Ø£Ø¨Ùˆ Ù‚ÙŠØ± Ù„Ù„Ø£Ø³Ù…Ø¯Ø©": "ABUK", "ÙÙˆØ±ÙŠ": "FWRY",
    "Ù…ØµØ± Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø£Ø³Ù…Ø¯Ø© - Ù…ÙˆØ¨ÙƒÙˆ": "MFPC", "Ù…ÙˆØ¨ÙƒÙˆ": "MFPC", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ© Ù„ØªØ¯Ø§ÙˆÙ„ Ø§Ù„Ø­Ø§ÙˆÙŠØ§Øª": "ALCN",
    "Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST", "Ø§ÙŠØ³ØªØ±Ù† ÙƒÙˆÙ…Ø¨Ø§Ù†ÙŠ": "EAST", "Ø¨Ø§Ù„Ù… Ù‡ÙŠÙ„Ø²": "PHDC",
    "Ø³ÙŠØ¯ÙŠ ÙƒØ±ÙŠØ± Ù„Ù„Ø¨ØªØ±ÙˆÙƒÙŠÙ…Ø§ÙˆÙŠØ§Øª": "SKPC", "Ø³ÙŠØ¯Ø¨Ùƒ": "SKPC", "Ø£ÙˆØ±Ø§Ø³ÙƒÙˆÙ… ÙƒÙˆÙ†Ø³ØªØ±Ø§ÙƒØ´ÙˆÙ†": "ORAS",
    "Ø¬ÙŠ Ø¨ÙŠ ÙƒÙˆØ±Ø¨": "AUTO", "Ø¥Ø¹Ù…Ø§Ø± Ù…ØµØ±": "EMFD", "Ù…ÙŠÙ†Ø§ Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø³ÙŠØ§Ø­ÙŠ ÙˆØ§Ù„Ø¹Ù‚Ø§Ø±ÙŠ": "MENA",
    "Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø®Ø²Ù ÙˆØ§Ù„ØµÙŠÙ†ÙŠ": "PRCL", "Ø¬Ù†ÙˆØ¨ Ø§Ù„ÙˆØ§Ø¯Ù‰ Ù„Ù„Ø£Ø³Ù…Ù†Øª": "SVCE",
    "Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© Ù„Ù„Ù…Ø­Ø§ØµÙŠÙ„ Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©": "IFAP", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø®Ø²Ù Ø³ÙŠØ±Ø§Ù…ÙŠÙƒØ§": "CERA",
    "Ø§Ù„Ø¹Ø² Ù„Ù„Ø³ÙŠØ±Ø§Ù…ÙŠÙƒ ÙˆØ§Ù„Ø¨ÙˆØ±Ø³Ù„ÙŠÙ†": "ECAP", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø­Ù„ÙŠØ¬ Ø§Ù„Ø£Ù‚Ø·Ø§Ù†": "ACGC",
    "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¹Ø§Ù…Ø± Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "AMER", "Ø§Ù„Ù†ØµØ± Ù„Ù„Ù…Ù„Ø§Ø¨Ø³ ÙˆØ§Ù„Ù…Ù†Ø³ÙˆØ¬Ø§Øª": "KABO",
    "Ø§Ù„Ù…Ø·ÙˆØ±ÙˆÙ† Ø§Ù„Ø¹Ø±Ø¨ Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "ARAB", "Ø·Ø§Ù‚Ø© Ø¹Ø±Ø¨ÙŠØ© Ø´.Ù….Ù…": "TAQA", "Ø§Ù„Ø¹Ø¨ÙˆØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ©": "MEPA",
    "Ø§Ù„Ù…ØµØ±Ù Ø§Ù„Ù…ØªØ­Ø¯": "UBEE", "Ø§Ù„Ø¹Ø¨ÙˆØ± Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ": "OBRI",
    "Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ": "RREI", "Ù…ØµØ±Ù Ø£Ø¨Ùˆ Ø¸Ø¨ÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ - Ù…ØµØ±": "ADIB",
    "Ù†Ù‡Ø± Ø§Ù„Ø®ÙŠØ± Ù„Ù„ØªÙ†Ù…ÙŠØ©": "KRDI", "Ù…Ù…ÙÙŠØ³ Ù„Ù„Ø£Ø¯ÙˆÙŠØ©": "MPCI", "Ù…ØµØ± Ù„Ù„Ø£Ù„ÙˆÙ…Ù†ÙŠÙˆÙ…": "EGAL",
    "Ù…ØµØ± Ù„Ù„Ø£Ø³Ù…Ù†Øª": "MCQE", "Ù…ØµØ± Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ù„Ù„ØµÙ„Ø¨": "ATQA", "Ù…ØµØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø§Ø³ÙƒØ§Ù†": "HELI",
    "Ù…Ø¯ÙŠÙ†Ø© Ù†ØµØ± Ù„Ù„Ø§Ø³ÙƒØ§Ù†": "MASR", "Ù…Ø§ÙƒØ±Ùˆ Ø¬Ø±ÙˆØ¨": "MCRO", "Ù„ÙŠØ³ÙŠÙƒÙˆ Ù…ØµØ±": "LCSW",
    "ÙƒÙˆÙ†ØªÙƒØª Ø§Ù„Ù…Ø§Ù„ÙŠØ©": "CNFN", "ÙØ§Ù„Ù…ÙˆØ± Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "VLMRA", "ØºØ§Ø² Ù…ØµØ±": "EGAS",
    "Ø¹Ø¨ÙˆØ± Ù„Ø§Ù†Ø¯": "OLFI", "Ù…Ø³ØªØ´ÙÙ‰ ÙƒÙ„ÙŠÙˆØ¨Ø§ØªØ±Ø§": "CLHO", "Ø§Ù„Ù‚Ù„Ø¹Ø© Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª": "CCAP",
    "Ø²Ù‡Ø±Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ø¯ÙŠ": "ZMID", "Ø±Ø§ÙŠØ© Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø©": "RAYA", "Ø¯Ø§ÙŠØ³ Ù„Ù„Ù…Ù„Ø§Ø¨Ø³": "DSCW",
    "Ø¬Ù‡ÙŠÙ†Ø© Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©": "JUFO", "Ø¨ÙŠ Ø¥Ù†ÙØ³ØªÙ…Ù†ØªØ³": "BINV", "ÙƒØ±ÙŠØ¯ÙŠ Ø£Ø¬Ø±ÙŠÙƒÙˆÙ„": "CIEB",
    "Ø¨Ù†Ùƒ Ø§Ù„ØªØ¹Ù…ÙŠØ± ÙˆØ§Ù„Ø¥Ø³ÙƒØ§Ù†": "HDBK", "Ø¨Ù„ØªÙˆÙ† Ø§Ù„Ù…Ø§Ù„ÙŠØ©": "BTFH", "Ø¨Ø§ÙŠÙˆÙ†ÙŠØ±Ø² Ø¨Ø±ÙˆØ¨Ø±ØªÙŠØ²": "PRDC",
    "Ø§ÙŠ ÙØ§ÙŠÙ†Ø§Ù†Ø³": "EFIH", "Ø§Ù….Ø§Ù… Ø¬Ø±ÙˆØ¨": "MTIE", "Ø§Ù„Ù†Ø³Ø§Ø¬ÙˆÙ† Ø§Ù„Ø´Ø±Ù‚ÙŠÙˆÙ†": "ORWE",
    "Ø§Ù„Ù…Ù†ØµÙˆØ±Ø© Ù„Ù„Ø¯ÙˆØ§Ø¬Ù†": "MPCO", "Ø§Ù„Ù…Ù„ØªÙ‚Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ": "AMIA", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ": "MPRC",
    "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù„Ù…Ù†ØªØ¬Ø¹Ø§Øª": "EGTS", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ù„Ø§ØªØµØ§Ù„Ø§Øª": "ETEL", "Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù†Ù‚Ù„": "ETRS",
    "Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© Ù„Ù„ØµÙ†Ø§Ø¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠØ©": "PHAR"
}

# --- Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© ---
FAIR_VALUES_FILE = "fair_values.json"
FAIR_VALUES_DB = {}

def load_fair_values():
    global FAIR_VALUES_DB
    if os.path.exists(FAIR_VALUES_FILE):
        try:
            with open(FAIR_VALUES_FILE, "r", encoding="utf-8") as f:
                FAIR_VALUES_DB = json.load(f)
                print(f"âœ… Loaded {len(FAIR_VALUES_DB)} fair value entries.")
        except Exception as e:
            print(f"âš ï¸ Error loading fair_values.json: {e}")
    else:
        print("âš ï¸ fair_values.json not found. Fair value data will be skipped.")

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Gemini Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ø­ØªÙŠØ§Ø·ÙŠ ---
model = None
selected_model_name = None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    MODEL_LIST = ["gemini-1.5-flash-latest", "gemini-1.5-pro-latest", "gemini-pro"]
    for m in MODEL_LIST:
        try:
            model = genai.GenerativeModel(m)
            selected_model_name = m
            print(f"âœ… Model initialized: {selected_model_name}")
            break
        except Exception as e:
            print(f"âš ï¸ Model {m} not available: {str(e)[:50]}")
            continue

# --- Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ---
RSS_FEEDS = [
    "https://www.arabfinance.com/ar/rss/rssbycat/2",
    "https://www.arabfinance.com/ar/rss/rssbycat/3",
    "http://feeds.mubasher.info/ar/EGX/news",
]
MUBASHER_PULSE_URL = "https://www.mubasher.info/news/eg/pulse/stocks"

# --- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙÙ„ØªØ±Ø© ---
STOCK_KEYWORDS = list(TICKER_MAP.keys()) + [
    "Ø³Ù‡Ù…", "Ø£Ø³Ù‡Ù…", "Ø¨ÙˆØ±ØµØ©", "Ø§Ø±Ø¨Ø§Ø­", "Ø£Ø±Ø¨Ø§Ø­", "Ø®Ø³Ø§Ø¦Ø±", "Ù†ØªØ§Ø¦Ø¬ Ø£Ø¹Ù…Ø§Ù„",
    "Ø²ÙŠØ§Ø¯Ø© Ø±Ø£Ø³ Ù…Ø§Ù„", "ØªÙˆØ²ÙŠØ¹ ÙƒÙˆØ¨ÙˆÙ†", "Ø§Ø³ØªØ­ÙˆØ§Ø°", "Ø§Ù†Ø¯Ù…Ø§Ø¬", "Ø§ÙƒØªØªØ§Ø¨",
    "Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…Ø¬Ù„Ø³ Ø¥Ø¯Ø§Ø±Ø©", "Ø¥ÙØµØ§Ø­", "ØªØ¯Ø§ÙˆÙ„", "Ø§Ù„Ø¨ÙˆØ±ØµØ© Ø§Ù„Ù…ØµØ±ÙŠØ©",
    "EGX", "ÙƒÙˆØ¨ÙˆÙ†", "Ø¬Ù…Ø¹ÙŠØ© Ø¹Ù…ÙˆÙ…ÙŠØ©", "Ù‡ÙŠØ¦Ø© Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…ÙˆØ§Ø²Ù†Ø©"
]

# --- Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø§Ù„Ø© ---
SENT_NEWS_DB_FILE = "sent_news_db.json"

# --- Ø¯ÙˆØ§Ù„ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ ---
def is_similar(title1, title2, threshold=0.85):
    return SequenceMatcher(None, title1, title2).ratio() >= threshold

def generate_news_hash(title, link):
    return hashlib.md5(f"{title.strip()}_{link.strip()}".encode("utf-8")).hexdigest()

# --- Ø¯ÙˆØ§Ù„ Ø¥Ø¯Ø§Ø±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def load_sent_news_db():
    if os.path.exists(SENT_NEWS_DB_FILE):
        try:
            with open(SENT_NEWS_DB_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"âš ï¸ Error loading DB: {e}. Starting fresh.")
            return {}
    return {}

def save_sent_news_db(db):
    try:
        keys_to_keep = list(db.keys())[-500:]
        trimmed_db = {k: db[k] for k in keys_to_keep}
        with open(SENT_NEWS_DB_FILE, "w", encoding="utf-8") as f:
            json.dump(trimmed_db, f, ensure_ascii=False, indent=2)
    except IOError as e:
        print(f"âš ï¸ Error saving DB: {e}")

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚ ---
def find_tickers(text):
    found_tickers = set()
    for company, ticker in TICKER_MAP.items():
        if company in text:
            found_tickers.add(f"#{ticker}")
    return list(found_tickers)

def get_fair_value_data(tickers):
    data = {}
    for ticker_tag in tickers:
        ticker = ticker_tag.replace("#", "")
        if ticker in FAIR_VALUES_DB:
            data[ticker] = FAIR_VALUES_DB[ticker]
    return data

def format_fair_value_for_prompt(fair_value_data):
    if not fair_value_data:
        return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚ÙŠÙ…Ø© Ø¹Ø§Ø¯Ù„Ø© Ù…ØªØ§Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨Ø±."
    formatted_data = []
    for ticker, data in fair_value_data.items():
        company_name = data.get('company_names', [ticker])[0]
        fv_val = data.get('fair_value')
        fv = f"{fv_val:.2f}" if fv_val is not None else "N/A"
        upside_val = data.get('upside_percent')
        upside = f"{upside_val:.1f}%" if upside_val is not None else "N/A"
        valuation = data.get('valuation', 'N/A')
        formatted_data.append(
            f"- {company_name} ({ticker}): Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø© {fv} Ø¬.Ù…ØŒ ÙØ±ØµØ© Ø§Ù„ØµØ¹ÙˆØ¯ {upside}ØŒ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„ÙŠ {valuation}."
        )
    return "\n".join(formatted_data)

async def analyze_news_with_gemini(title, fair_value_data):
    if not model:
        return None

    prompt = f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ø§Ù„ÙŠ Ø±Ù‚Ù…ÙŠ Ø³Ø±ÙŠØ¹. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ Ø¨Ø·Ø§Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹Ø© Ù„Ù„Ù…Ø³ØªØ«Ù…Ø±.

**Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:**
- **Ø§Ù„Ø®Ø¨Ø±:** {title}
- **Ø¨ÙŠØ§Ù†Ø§Øª Investing Pro:**
{format_fair_value_for_prompt(fair_value_data)}

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ø®ØªØµØ± Ø¬Ø¯Ø§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø§Ù„Ø¶Ø¨Ø·):**
Ø§Ù„Ø®Ù„Ø§ØµØ©:
â€¢ [Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ù†Ù‚Ø·Ø© Ù…ÙˆØ¬Ø²Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù† ØªØ£Ø«ÙŠØ± Ø§Ù„Ø®Ø¨Ø±]
â€¢ [Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ù†Ù‚Ø·Ø© Ø±Ù‚Ù…ÙŠØ© Ø¥Ù† Ø£Ù…ÙƒÙ†ØŒ Ù…Ø«Ø§Ù„: Ø²ÙŠØ§Ø¯Ø© Ù…ØªÙˆÙ‚Ø¹Ø© 10%]
â€¢ [Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ù†Ù‚Ø·Ø© Ø¹Ù† Ø§Ù„Ø´Ø¹ÙˆØ± Ø§Ù„Ø¹Ø§Ù…: Ø¥ÙŠØ¬Ø§Ø¨ÙŠ/Ø³Ù„Ø¨ÙŠ/Ù…Ø­Ø§ÙŠØ¯]
Ù…Ø¤Ø´Ø± Ø§Ù„Ø«Ù‚Ø©: [Ø±Ù‚Ù… Ù…Ù† 1 Ø¥Ù„Ù‰ 10]
Ù‚Ø±Ø§Ø± Ø§Ù„Ù…Ø­Ù„Ù„: [Ø´Ø±Ø§Ø¡/Ø§Ø­ØªÙØ§Ø¸/Ø¨ÙŠØ¹] | Ø§Ù„Ù‡Ø¯Ù: [Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù ÙƒØ±Ù‚Ù…]
"""

    try:
        response = await model.generate_content_async(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"âš ï¸ Gemini analysis failed: {e}")
        return None

def is_stock_related(title, description):
    content = (title + " " + description).lower()
    return any(keyword.lower() in content for keyword in STOCK_KEYWORDS)

async def process_and_send(bot, title, description, link, sent_db):
    if not is_stock_related(title, description):
        return False, "Not stock related"

    news_hash = generate_news_hash(title, link)
    if news_hash in sent_db:
        return False, "Duplicate hash"

    for existing_hash, existing_data in sent_db.items():
        if is_similar(title, existing_data["title"]):
            return False, "Similar title"

    tickers = find_tickers(title + " " + description)
    if not tickers:
        return False, "No tickers found"

    fair_value_data = get_fair_value_data(tickers)
    if not fair_value_data:
        return False, "No fair value data for these tickers"

    print(f"ğŸ“° Processing: {title[:60]}...")
    analysis_text = await analyze_news_with_gemini(title, fair_value_data)

    # Parsing the structured analysis
    summary_points = re.findall(r"â€¢\s*(.*)", analysis_text) if analysis_text else []
    confidence_match = re.search(r"Ù…Ø¤Ø´Ø± Ø§Ù„Ø«Ù‚Ø©:\s*(\d+)", analysis_text) if analysis_text else None
    decision_match = re.search(r"Ù‚Ø±Ø§Ø± Ø§Ù„Ù…Ø­Ù„Ù„:\s*(.*)", analysis_text) if analysis_text else None
    
    confidence_score = confidence_match.group(1) if confidence_match else "N/A"
    analyst_decision = decision_match.group(1) if decision_match else "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù‚ÙŠØ¯ Ø§Ù„ØªØ­Ø¯ÙŠØ«"

    # Build the message
    for ticker_tag in tickers:
        ticker = ticker_tag.replace("#", "")
        if ticker in fair_value_data:
            data = fair_value_data[ticker]
            company_name = data.get('company_names', [ticker])[0]
            curr_p_val = data.get('current_price')
            current_price = f"{curr_p_val:.2f}" if curr_p_val is not None else "N/A"
            fv_val = data.get('fair_value')
            fv = f"{fv_val:.2f}" if fv_val is not None else "N/A"
            upside_percent = data.get('upside_percent', 0)
            upside_icon = "ğŸ“ˆ" if upside_percent > 0 else ("ğŸ“‰" if upside_percent < 0 else "â†”ï¸")
            upside_val = f"{abs(upside_percent):.1f}%"

            message = (
                f"ğŸ›ï¸ <b>ØªØ­Ù„ÙŠÙ„ Ø³Ù‡Ù…: {company_name} ({ticker})</b>\n"
                f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"ğŸ“Œ <b>Ø§Ù„Ø®Ø¨Ø±:</b> {title}\n\n"
                f"ğŸ“Š <b>Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠ (Investing Pro):</b>\n"
                f"  - Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ: {current_price} Ø¬.Ù…\n"
                f"  - Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¯Ù„Ø©: {fv} Ø¬.Ù…\n"
                f"  - ÙØ±ØµØ© Ø§Ù„ØµØ¹ÙˆØ¯: {upside_icon} {upside_val}\n\n"
                f"ğŸ’¡ <b>Ø§Ù„Ø®Ù„Ø§ØµØ© ÙÙŠ 3 Ù†Ù‚Ø§Ø·:</b>\n"
                f"  â€¢ {summary_points[0] if len(summary_points) > 0 else '...'}\n"
                f"  â€¢ {summary_points[1] if len(summary_points) > 1 else '...'}\n"
                f"  â€¢ {summary_points[2] if len(summary_points) > 2 else '...'}\n\n"
                f"ğŸ¯ <b>Ù‚Ø±Ø§Ø± Ø§Ù„Ù…Ø­Ù„Ù„ (Ø§Ù„Ø«Ù‚Ø©: {confidence_score}/10):</b>\n"
                f"  - {analyst_decision}\n"
                f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"<a href=\"{link}\">Ø§Ù„Ù…ØµØ¯Ø±</a> | {ticker_tag}"
            )

            try:
                await bot.send_message(chat_id=CHANNEL_ID, text=message, parse_mode=ParseMode.HTML)
                sent_db[news_hash] = {"title": title, "link": link}
                print(f"âœ… Sent analysis for {ticker}")
                await asyncio.sleep(2) # Delay between messages
            except Exception as e:
                print(f"âŒ Error sending message for {ticker}: {e}")

    return True, "Processed"

async def fetch_rss_feeds(bot, sent_db):
    headers = {"User-Agent": "Mozilla/5.0"}
    for url in RSS_FEEDS:
        try:
            response = requests.get(url, headers=headers, timeout=15)
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:15]:
                await process_and_send(bot, entry.title, entry.get("summary", ""), entry.link, sent_db)
        except Exception as e:
            print(f"âš ï¸ Error fetching RSS {url[:50]}: {e}")

async def main():
    if not all([TELEGRAM_TOKEN, CHANNEL_ID, GEMINI_API_KEY]):
        print("âŒ Missing environment variables!")
        return

    bot = Bot(token=TELEGRAM_TOKEN)
    sent_db = load_sent_news_db()
    load_fair_values()

    print(f"\nğŸ¤– EGX News Bot v6.0 Started | Model: {selected_model_name or 'N/A'} | Tracked: {len(sent_db)}")

    await fetch_rss_feeds(bot, sent_db)

    save_sent_news_db(sent_db)
    print(f"\nâœ… Cycle completed. Total tracked news: {len(sent_db)}\n")

if __name__ == "__main__":
    asyncio.run(main())
