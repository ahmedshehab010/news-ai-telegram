import feedparser
import asyncio
import os
import json
import requests
import random
from bs4 import BeautifulSoup
from openai import OpenAI
from telegram import Bot
from telegram.constants import ParseMode

# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù„Ù„Ø­Ù…Ø§ÙŠØ© ÙÙŠ GitHub)
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
RSS_FEEDS = [
    "https://www.arabfinance.com/ar/rss/rssbycat/2",
    "https://www.arabfinance.com/ar/rss/rssbycat/3",
    "http://feeds.mubasher.info/ar/EGX/news",
]
MUBASHER_PULSE_URL = "https://www.mubasher.info/news/eg/pulse/stocks"

STOCK_KEYWORDS = [
    "Ø³Ù‡Ù…", "Ø£Ø³Ù‡Ù…", "Ø¨ÙˆØ±ØµØ©", "Ø§Ø±Ø¨Ø§Ø­", "Ø£Ø±Ø¨Ø§Ø­", "Ø®Ø³Ø§Ø¦Ø±", "Ù†ØªØ§Ø¦Ø¬ Ø£Ø¹Ù…Ø§Ù„", 
    "Ø²ÙŠØ§Ø¯Ø© Ø±Ø£Ø³ Ù…Ø§Ù„", "ØªÙˆØ²ÙŠØ¹ ÙƒÙˆØ¨ÙˆÙ†", "Ø§Ø³ØªØ­ÙˆØ§Ø°", "Ø§Ù†Ø¯Ù…Ø§Ø¬", "Ø§ÙƒØªØªØ§Ø¨", 
    "Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…Ø¬Ù„Ø³ Ø¥Ø¯Ø§Ø±Ø©", "Ø¥ÙØµØ§Ø­", "ØªØ¯Ø§ÙˆÙ„", "Ø§Ù„Ø¨ÙˆØ±ØµØ© Ø§Ù„Ù…ØµØ±ÙŠØ©",
    "EGX", "ÙƒÙˆØ¨ÙˆÙ†", "Ø¬Ù…Ø¹ÙŠØ© Ø¹Ù…ÙˆÙ…ÙŠØ©", "Ù‡ÙŠØ¦Ø© Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©", "Ù…ÙˆØ§Ø²Ù†Ø©"
]

SENT_NEWS_FILE = "sent_news.json"
AI_MODELS = ["gpt-4.1-mini", "gpt-4.1-nano", "gemini-2.5-flash"]

client = OpenAI(api_key=OPENAI_API_KEY)

async def analyze_news(title, description):
    selected_model = random.choice(AI_MODELS)
    prompt = f"""
    Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ø§Ù„ÙŠ Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Fundamental Analysis) Ø¨Ø§Ù„Ø¨ÙˆØ±ØµØ© Ø§Ù„Ù…ØµØ±ÙŠØ©. 
    Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø±Ø¤ÙŠØ© Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ© Ù…Ø®ØªØµØ±Ø©:
    Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title}
    Ø§Ù„ØªÙØ§ØµÙŠÙ„: {description}
    
    Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„:
    1. **ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ Ø³Ø±ÙŠØ¹**: (ÙƒÙŠÙ ÙŠØ¤Ø«Ø± Ø§Ù„Ø®Ø¨Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø§Ù„ÙŠØ©ØŒ Ø§Ù„Ø±Ø¨Ø­ÙŠØ©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ù„Ø§Ø¡Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ù„Ù„Ø´Ø±ÙƒØ©ØŸ).
    2. **ØªØ£Ø«ÙŠØ± Ø§Ù„Ø®Ø¨Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ©**: (Ù‡Ù„ Ù‡Ùˆ Ù…Ø­ÙØ² Ù„Ù„Ù†Ù…ÙˆØŒ Ø£Ù… Ù…Ø®Ø§Ø·Ø±Ø© ØªØ´ØºÙŠÙ„ÙŠØ©ØŒ Ø£Ù… Ø¥Ø¬Ø±Ø§Ø¡ Ø±ÙˆØªÙŠÙ†ÙŠØŸ).
    3. **Ø§Ù„ØªÙ‚ÙŠÙŠÙ…**: (Ø¥ÙŠØ¬Ø§Ø¨ÙŠ / Ø³Ù„Ø¨ÙŠ / Ù…ØªØ¹Ø§Ø¯Ù„) Ù…Ø¹ Ø°ÙƒØ± Ø§Ù„Ø³Ø¨Ø¨ Ø¨Ø§Ø®ØªØµØ§Ø±.
    4. **Ù†ØµÙŠØ­Ø© Ù„Ù„Ù…Ø³ØªØ«Ù…Ø±**: (Ù…Ø§Ø°Ø§ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙØ¹Ù„ Ø­Ø§Ù…Ù„ Ø§Ù„Ø³Ù‡Ù… Ø£Ùˆ Ø§Ù„Ø±Ø§ØºØ¨ ÙÙŠ Ø§Ù„Ø´Ø±Ø§Ø¡ØŸ).

    Ø§Ø¬Ø¹Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ù‡Ù†ÙŠØŒ ÙˆØ¨Ø´ÙƒÙ„ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø®ØªØµØ±Ø© Ø¬Ø¯Ø§Ù‹.
    """
    try:
        response = client.chat.completions.create(
            model=selected_model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ ØªØ¹Ø°Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ."

def is_stock_related(title, description):
    content = (title + " " + description).lower()
    return any(keyword in content for keyword in STOCK_KEYWORDS)

def load_sent_news():
    if os.path.exists(SENT_NEWS_FILE):
        with open(SENT_NEWS_FILE, "r") as f:
            try: return json.load(f)
            except: return []
    return []

def save_sent_news(sent_news):
    with open(SENT_NEWS_FILE, "w") as f:
        json.dump(sent_news[-500:], f)

async def process_and_send(bot, news_id, title, description, link, sent_news):
    if news_id not in sent_news and is_stock_related(title, description):
        print(f"Processing: {title}")
        analysis = await analyze_news(title, description)
        message = (
            f"<b>ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø³Ù‡Ù…: {title}</b>\n\n"
            f"ğŸ“ <b>Ø§Ù„Ø®Ø¨Ø±:</b> {description}\n\n"
            f"ğŸ” <b>Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (AI):</b>\n{analysis}\n\n"
            f"ğŸ”— <a href='{link}'>Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„</a>"
        )
        try:
            await bot.send_message(chat_id=CHANNEL_ID, text=message, parse_mode=ParseMode.HTML)
            sent_news.append(news_id)
            return True
        except Exception as e:
            print(f"Error sending: {e}")
    return False

async def scrape_mubasher_pulse(bot, sent_news):
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        response = requests.get(MUBASHER_PULSE_URL, headers=headers, timeout=20)
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('a', class_='mi-article-list-item__title') or soup.find_all('a', href=True)
        count = 0
        for a in articles:
            title = a.get_text(strip=True)
            link = a['href']
            if not link.startswith('http'): link = "https://www.mubasher.info" + link
            news_id = link.split('/')[-1] or link
            if news_id not in sent_news and "/news/" in link and len(title) > 20:
                if await process_and_send(bot, news_id, title, "Ø®Ø¨Ø± Ø¹Ø§Ø¬Ù„ Ù…Ù† Ù†Ø¨Ø¶ Ø§Ù„Ø£Ø³Ù‡Ù….", link, sent_news):
                    count += 1
                    await asyncio.sleep(2)
            if count >= 5: break
    except: pass

async def main():
    if not all([TELEGRAM_TOKEN, CHANNEL_ID, OPENAI_API_KEY]):
        print("Missing environment variables!")
        return
        
    bot = Bot(token=TELEGRAM_TOKEN)
    sent_news = load_sent_news()
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for feed_url in RSS_FEEDS:
        try:
            response = requests.get(feed_url, headers=headers, timeout=20)
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:10]:
                news_id = entry.get("guid", entry.link)
                await process_and_send(bot, news_id, entry.title, entry.get("description", ""), entry.link, sent_news)
        except: pass
    
    await scrape_mubasher_pulse(bot, sent_news)
    save_sent_news(sent_news)

if __name__ == "__main__":
    asyncio.run(main())
